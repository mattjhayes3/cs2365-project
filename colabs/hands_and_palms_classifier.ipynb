{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9d9363e22a704cd1b2d954ac6c25e218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e258fd9a42de4dc8a25c6038db466e6f",
              "IPY_MODEL_d64925a4eaed4ce1812d544516e171db",
              "IPY_MODEL_9bb7f1712ccb4e37b4f7d1009296506c"
            ],
            "layout": "IPY_MODEL_6bc8094c9f6a42ea9408bdb6af9c2791"
          }
        },
        "e258fd9a42de4dc8a25c6038db466e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e11d0ccbdd1d4f409fe9a0a66a1e98fa",
            "placeholder": "​",
            "style": "IPY_MODEL_4d6f7acdbba84a26a6f345cff582730b",
            "value": "tf_model.h5: 100%"
          }
        },
        "d64925a4eaed4ce1812d544516e171db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_344a3e8d585743379d4957a9d076c733",
            "max": 345823776,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3583f27c5d29423d8de674ecbb85d29d",
            "value": 345823776
          }
        },
        "9bb7f1712ccb4e37b4f7d1009296506c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94f45ce875594684807ca095be67f0d3",
            "placeholder": "​",
            "style": "IPY_MODEL_4f0105920032440599320dabffee9f08",
            "value": " 346M/346M [00:01&lt;00:00, 236MB/s]"
          }
        },
        "6bc8094c9f6a42ea9408bdb6af9c2791": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e11d0ccbdd1d4f409fe9a0a66a1e98fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d6f7acdbba84a26a6f345cff582730b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "344a3e8d585743379d4957a9d076c733": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3583f27c5d29423d8de674ecbb85d29d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94f45ce875594684807ca095be67f0d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f0105920032440599320dabffee9f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RbC8tYD_P8h",
        "outputId": "1b506a6c-32cc-4567-83f8-57ee84047dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m928.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, responses, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 evaluate-0.4.1 multiprocess-0.70.15 pyarrow-hotfix-0.6 responses-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q mediapipe==0.10.0\n",
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\n",
        "!pip install --upgrade -q gspread\n",
        "!pip install datasets evaluate\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "import pandas as pd\n",
        "auth.authenticate_user()\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiRQR25QEKmi",
        "outputId": "e7dc8d63-ef1b-4353-af34-bddb88648a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "worksheet = gc.open('hands and palms loras').sheet1.get(\"A2:I625\")\n",
        "df = pd.DataFrame.from_records(worksheet)\n",
        "df = df.rename({0:'rank', 1:'checkpoint', 2:'side',3:'gender', 4:'hand', 5:'imagenum', 6: 'bad', 7:'ok', 8:'good'}, axis=1)\n",
        "df.loc[df.good=='1', ['label']] = 'goodhand'\n",
        "df.loc[df.bad=='1', ['label']] = 'badhand'\n",
        "df.loc[(df.label=='goodhand') | (df.label=='badhand')]\n",
        "df['image'] = None\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n9nnWoWYDEKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['good'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxqdRHnNllZ3",
        "outputId": "1be313ea-29c0-48e4-e8c2-71e1f3706977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    169\n",
              "Name: good, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['bad'].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxKTxSMplu0C",
        "outputId": "a3974444-c006-4535-aa79-2c4977e157a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     383\n",
              "1    241\n",
              "Name: bad, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nfu77wSRiJSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import Dataset\n",
        "from datasets import DatasetDict\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
        "from tqdm import tqdm\n",
        "\n",
        "id2label = {1:'goodhand', 0:'badhand'}\n",
        "label2id = {v:k for k,v in id2label.items()}\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "from datasets import Dataset\n",
        "\n",
        "data = defaultdict(list)\n",
        "\n",
        "examples = df.loc[(df.label=='goodhand') | (df.label=='badhand')]\n",
        "for index, row in tqdm([(index, row) for index, row in examples.iterrows()]):\n",
        "  if row['rank'] != 'None':\n",
        "    path = f'/content/drive/MyDrive/LoRA/inference/hands and palms r{row[\"rank\"]} lora/{row[\"side\"]}_of_fair-skinned_{row[\"gender\"]}_{row[\"hand\"]}_hand_chkpt{row[\"checkpoint\"]}_steps30_{row[\"imagenum\"]}.png'\n",
        "  else:\n",
        "    path = f'/content/drive/MyDrive/LoRA/inference/hands and palms r{row[\"rank\"]} lora/{row[\"side\"]}_of_fair-skinned_{row[\"gender\"]}_{row[\"hand\"]}_hand_chkpt5000_steps30_{row[\"imagenum\"]}.png'\n",
        "  data['image'].append(Image.open(path))\n",
        "  data['label'].append(label2id[row['label']])\n",
        "data = Dataset.from_dict(data)\n",
        "\n",
        "\n",
        "\n",
        "# data = Dataset.from_pandas(df[['image', 'label']])\n",
        "train_test = data.train_test_split(test_size=0.15, seed=1234)\n",
        "test_valid = train_test['test'].train_test_split(test_size=0.5, seed=1234)\n",
        "train_test_valid_dataset = DatasetDict({\n",
        "    'train': train_test['train'],\n",
        "    'test': test_valid['test'],\n",
        "    'valid': test_valid['train']})\n",
        "labels = ['goodhand', 'badhand']\n",
        "\n",
        "\n",
        "from transformers import AutoImageProcessor\n",
        "\n",
        "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "# normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "# _transforms = Compose([ToTensor(), normalize])\n",
        "\n",
        "\n",
        "# def transforms(examples):\n",
        "#     examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
        "#     del examples[\"image\"]\n",
        "#     return examples\n",
        "\n",
        "# train_test_valid_dataset = train_test_valid_dataset.with_transform(transform)\n",
        "\n",
        "# from transformers import DefaultDataCollator\n",
        "\n",
        "# data_collator = DefaultDataCollator()\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
        "\n",
        "train_data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomCrop(size[0], size[1]),\n",
        "        # layers.Resizing(size[0], size[1]),\n",
        "        layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(height_factor=0.1, width_factor=0.1),\n",
        "    ],\n",
        "    name=\"train_data_augmentation\",\n",
        ")\n",
        "\n",
        "val_data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        # layers.Resizing(size[0], size[1]),\n",
        "        layers.CenterCrop(size[0], size[1]),\n",
        "        layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
        "    ],\n",
        "    name=\"val_data_augmentation\",\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def convert_to_tf_tensor(image: Image):\n",
        "    np_image = np.array(image)\n",
        "    tf_image = tf.convert_to_tensor(np_image)\n",
        "    # `expand_dims()` is used to add a batch dimension since\n",
        "    # the TF augmentation layers operates on batched inputs.\n",
        "    return tf.expand_dims(tf_image, 0)\n",
        "\n",
        "\n",
        "def preprocess_train(example_batch):\n",
        "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
        "    images = [\n",
        "        train_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
        "    ]\n",
        "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
        "    return example_batch\n",
        "\n",
        "\n",
        "def preprocess_val(example_batch):\n",
        "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
        "    images = [\n",
        "        val_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
        "    ]\n",
        "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
        "    return example_batch\n",
        "\n",
        "train_test_valid_dataset[\"train\"].set_transform(preprocess_train)\n",
        "train_test_valid_dataset[\"test\"].set_transform(preprocess_val)\n",
        "train_test_valid_dataset[\"valid\"].set_transform(preprocess_val)\n",
        "\n",
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
        "\n",
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "\n",
        "# model = AutoModelForImageClassification.from_pretrained(\n",
        "#     checkpoint,\n",
        "#     num_labels=len(labels),\n",
        "#     id2label=id2label,\n",
        "#     label2id=label2id,\n",
        "# )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffCWEY5ASsYQ",
        "outputId": "9e0c4d12-9b11-4239-b9f6-db96a2ef85dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 410/410 [00:00<00:00, 539.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "batch_size = 24\n",
        "num_epochs = 50\n",
        "num_train_steps = len(train_test_valid_dataset[\"train\"]) * num_epochs\n",
        "learning_rate = 1e-5\n",
        "weight_decay_rate = 0\n",
        "\n",
        "optimizer, lr_schedule = create_optimizer(\n",
        "    init_lr=learning_rate,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=weight_decay_rate,\n",
        "    num_warmup_steps=0,\n",
        ")\n",
        "\n",
        "from transformers import TFAutoModelForImageClassification\n",
        "\n",
        "model = TFAutoModelForImageClassification.from_pretrained(\n",
        "    checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# converting our train dataset to tf.data.Dataset\n",
        "tf_train_dataset = train_test_valid_dataset[\"train\"].to_tf_dataset(\n",
        "    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "# converting our test dataset to tf.data.Dataset\n",
        "tf_eval_dataset = train_test_valid_dataset[\"valid\"].to_tf_dataset(\n",
        "    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n",
        "\n",
        "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_eval_dataset)\n",
        "push_to_hub_callback = PushToHubCallback(\n",
        "    output_dir=\"hands_palms_classifier_resize\",\n",
        "    tokenizer=image_processor,\n",
        "    save_strategy=\"epoch\",\n",
        "    hub_token='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',\n",
        ")\n",
        "class CustomSaver(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.model.save(f\"/content/drive/MyDrive/cs236 project/hands_and_palms_classifier/model_{epoch}.hd5\".format(epoch))\n",
        "callbacks = [metric_callback, push_to_hub_callback]#CustomSaver()\n",
        "# callbacks = [metric_callback]\n",
        "\n",
        "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs, callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9d9363e22a704cd1b2d954ac6c25e218",
            "e258fd9a42de4dc8a25c6038db466e6f",
            "d64925a4eaed4ce1812d544516e171db",
            "9bb7f1712ccb4e37b4f7d1009296506c",
            "6bc8094c9f6a42ea9408bdb6af9c2791",
            "e11d0ccbdd1d4f409fe9a0a66a1e98fa",
            "4d6f7acdbba84a26a6f345cff582730b",
            "344a3e8d585743379d4957a9d076c733",
            "3583f27c5d29423d8de674ecbb85d29d",
            "94f45ce875594684807ca095be67f0d3",
            "4f0105920032440599320dabffee9f08"
          ]
        },
        "id": "JUPGV3UBcP4l",
        "outputId": "98d8a39b-a52f-4315-91e9-7c86fa4bc17b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tf_model.h5:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d9363e22a704cd1b2d954ac6c25e218"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/bias:0', 'vit/pooler/dense/kernel:0']\n",
            "- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
            "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Cloning https://huggingface.co/DownwardSpiral33/hands_palms_classifier_resize into local empty directory.\n",
            "WARNING:huggingface_hub.repository:Cloning https://huggingface.co/DownwardSpiral33/hands_palms_classifier_resize into local empty directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "15/15 [==============================] - 91s 3s/step - loss: 0.6821 - val_loss: 0.6453 - accuracy: 0.6452\n",
            "Epoch 2/50\n",
            "15/15 [==============================] - 57s 4s/step - loss: 0.6637 - val_loss: 0.6372 - accuracy: 0.6452\n",
            "Epoch 3/50\n",
            "15/15 [==============================] - 62s 4s/step - loss: 0.6512 - val_loss: 0.6279 - accuracy: 0.6452\n",
            "Epoch 4/50\n",
            "15/15 [==============================] - 63s 4s/step - loss: 0.6342 - val_loss: 0.6190 - accuracy: 0.7097\n",
            "Epoch 5/50\n",
            "15/15 [==============================] - 62s 4s/step - loss: 0.6092 - val_loss: 0.6068 - accuracy: 0.7097\n",
            "Epoch 6/50\n",
            "15/15 [==============================] - 59s 4s/step - loss: 0.5795 - val_loss: 0.5944 - accuracy: 0.7742\n",
            "Epoch 7/50\n",
            "15/15 [==============================] - 60s 4s/step - loss: 0.5557 - val_loss: 0.5840 - accuracy: 0.7742\n",
            "Epoch 8/50\n",
            "15/15 [==============================] - 68s 5s/step - loss: 0.5159 - val_loss: 0.5725 - accuracy: 0.7419\n",
            "Epoch 9/50\n",
            "15/15 [==============================] - 69s 5s/step - loss: 0.4782 - val_loss: 0.5667 - accuracy: 0.7419\n",
            "Epoch 10/50\n",
            "15/15 [==============================] - 61s 4s/step - loss: 0.4409 - val_loss: 0.5991 - accuracy: 0.6774\n",
            "Epoch 11/50\n",
            "15/15 [==============================] - 65s 4s/step - loss: 0.4026 - val_loss: 0.6284 - accuracy: 0.6129\n",
            "Epoch 12/50\n",
            "15/15 [==============================] - 58s 4s/step - loss: 0.3717 - val_loss: 0.6061 - accuracy: 0.6774\n",
            "Epoch 13/50\n",
            "15/15 [==============================] - 59s 4s/step - loss: 0.3223 - val_loss: 0.6154 - accuracy: 0.6452\n",
            "Epoch 14/50\n",
            "15/15 [==============================] - 59s 4s/step - loss: 0.2490 - val_loss: 0.6326 - accuracy: 0.7419\n",
            "Epoch 15/50\n",
            "15/15 [==============================] - 66s 5s/step - loss: 0.2393 - val_loss: 0.6448 - accuracy: 0.6452\n",
            "Epoch 16/50\n",
            "15/15 [==============================] - 60s 4s/step - loss: 0.1971 - val_loss: 0.6517 - accuracy: 0.6774\n",
            "Epoch 17/50\n",
            "15/15 [==============================] - 63s 4s/step - loss: 0.1856 - val_loss: 0.6966 - accuracy: 0.5806\n",
            "Epoch 18/50\n",
            "15/15 [==============================] - 65s 4s/step - loss: 0.1828 - val_loss: 0.7499 - accuracy: 0.6774\n",
            "Epoch 19/50\n",
            "15/15 [==============================] - 67s 5s/step - loss: 0.1416 - val_loss: 0.6842 - accuracy: 0.7097\n",
            "Epoch 20/50\n",
            "15/15 [==============================] - 67s 5s/step - loss: 0.1379 - val_loss: 0.6103 - accuracy: 0.7742\n",
            "Epoch 21/50\n",
            "15/15 [==============================] - 64s 4s/step - loss: 0.1395 - val_loss: 0.7928 - accuracy: 0.6774\n",
            "Epoch 22/50\n",
            "15/15 [==============================] - 68s 5s/step - loss: 0.1494 - val_loss: 0.6574 - accuracy: 0.7097\n",
            "Epoch 23/50\n",
            "15/15 [==============================] - 62s 4s/step - loss: 0.1075 - val_loss: 0.6905 - accuracy: 0.7097\n",
            "Epoch 24/50\n",
            "15/15 [==============================] - 70s 5s/step - loss: 0.0969 - val_loss: 0.6674 - accuracy: 0.7097\n",
            "Epoch 25/50\n",
            "15/15 [==============================] - 66s 5s/step - loss: 0.1016 - val_loss: 0.8444 - accuracy: 0.6452\n",
            "Epoch 26/50\n",
            "15/15 [==============================] - 64s 4s/step - loss: 0.0974 - val_loss: 0.9260 - accuracy: 0.6129\n",
            "Epoch 27/50\n",
            "15/15 [==============================] - 57s 4s/step - loss: 0.1400 - val_loss: 0.8985 - accuracy: 0.6774\n",
            "Epoch 28/50\n",
            "15/15 [==============================] - 60s 4s/step - loss: 0.0977 - val_loss: 0.9014 - accuracy: 0.5806\n",
            "Epoch 29/50\n",
            "15/15 [==============================] - 57s 4s/step - loss: 0.0759 - val_loss: 0.8310 - accuracy: 0.6452\n",
            "Epoch 30/50\n",
            "15/15 [==============================] - 66s 5s/step - loss: 0.0683 - val_loss: 0.8327 - accuracy: 0.6774\n",
            "Epoch 31/50\n",
            "15/15 [==============================] - 60s 4s/step - loss: 0.0670 - val_loss: 0.8247 - accuracy: 0.7097\n",
            "Epoch 32/50\n",
            "15/15 [==============================] - 67s 5s/step - loss: 0.0692 - val_loss: 0.8734 - accuracy: 0.6452\n",
            "Epoch 33/50\n",
            "15/15 [==============================] - 60s 4s/step - loss: 0.0701 - val_loss: 0.8369 - accuracy: 0.7097\n",
            "Epoch 34/50\n",
            "15/15 [==============================] - 65s 4s/step - loss: 0.0605 - val_loss: 1.0525 - accuracy: 0.6129\n",
            "Epoch 35/50\n",
            "15/15 [==============================] - 66s 5s/step - loss: 0.0847 - val_loss: 0.9439 - accuracy: 0.5806\n",
            "Epoch 36/50\n",
            "15/15 [==============================] - 59s 4s/step - loss: 0.0515 - val_loss: 0.8494 - accuracy: 0.6774\n",
            "Epoch 37/50\n",
            "15/15 [==============================] - 65s 4s/step - loss: 0.0584 - val_loss: 0.9270 - accuracy: 0.5806\n",
            "Epoch 38/50\n",
            "15/15 [==============================] - 66s 5s/step - loss: 0.0623 - val_loss: 1.0442 - accuracy: 0.5806\n",
            "Epoch 39/50\n",
            "15/15 [==============================] - 65s 5s/step - loss: 0.0532 - val_loss: 1.0707 - accuracy: 0.6129\n",
            "Epoch 40/50\n",
            "15/15 [==============================] - 64s 4s/step - loss: 0.0596 - val_loss: 0.9569 - accuracy: 0.6452\n",
            "Epoch 41/50\n",
            "15/15 [==============================] - 64s 4s/step - loss: 0.0529 - val_loss: 0.9502 - accuracy: 0.7097\n",
            "Epoch 42/50\n",
            "15/15 [==============================] - 64s 4s/step - loss: 0.0451 - val_loss: 0.9873 - accuracy: 0.6452\n",
            "Epoch 43/50\n",
            "15/15 [==============================] - 60s 4s/step - loss: 0.0465 - val_loss: 1.0143 - accuracy: 0.6774\n",
            "Epoch 44/50\n",
            "15/15 [==============================] - 64s 4s/step - loss: 0.0426 - val_loss: 1.0763 - accuracy: 0.6452\n",
            "Epoch 45/50\n",
            "15/15 [==============================] - 67s 5s/step - loss: 0.0445 - val_loss: 1.0282 - accuracy: 0.6452\n",
            "Epoch 46/50\n",
            "15/15 [==============================] - 62s 4s/step - loss: 0.0644 - val_loss: 1.0912 - accuracy: 0.6129\n",
            "Epoch 47/50\n",
            "15/15 [==============================] - 61s 4s/step - loss: 0.1811 - val_loss: 0.9771 - accuracy: 0.6774\n",
            "Epoch 48/50\n",
            "15/15 [==============================] - 61s 4s/step - loss: 0.1052 - val_loss: 1.0571 - accuracy: 0.6452\n",
            "Epoch 49/50\n",
            "15/15 [==============================] - 63s 4s/step - loss: 0.0741 - val_loss: 1.1330 - accuracy: 0.5484\n",
            "Epoch 50/50\n",
            "15/15 [==============================] - 64s 4s/step - loss: 0.0501 - val_loss: 1.0093 - accuracy: 0.6774\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f6760991ab0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForImageClassification\n",
        "\n",
        "model = TFAutoModelForImageClassification.from_pretrained(\n",
        "    'DownwardSpiral33/hands_palms_classifier_rot02',\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    # revision='acd88682d11a9bae028584b73817ddb26ae6d7a9', # 0.7096774193548387\n",
        "    revision='0192149ee2e394132560a65588bea9de138d4d0c', #0.7419354838709677 Test / 0.8387096774193549 Valid\n",
        ")\n",
        "results = ([], [])\n",
        "for x in train_test_valid_dataset[\"test\"]:\n",
        "  # print('pix', x['pixel_values'].shape)\n",
        "  # print('proc', image_processor(x['image'], return_tensors='tf')['pixel_values'].shape)\n",
        "  results[0].append(model({'pixel_values': tf.expand_dims(x['pixel_values'], axis=0)}).logits.numpy())\n",
        "  results[1].append(x['label'])\n",
        "results = (np.array(results[0])[:,0,:], np.array(results[1]))\n",
        "print(compute_metrics(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ckFo_sclZNi",
        "outputId": "2450b18b-700b-4c15-f2f4-614f43b45343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFViTForImageClassification.\n",
            "\n",
            "All the layers of TFViTForImageClassification were initialized from the model checkpoint at DownwardSpiral33/hands_palms_classifier_rot02.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.7419354838709677}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XnJjP1LHdnq8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}