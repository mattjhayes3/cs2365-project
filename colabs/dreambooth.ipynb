{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdwl4lf50sxJ",
        "outputId": "843b9e9f-9c71-440c-9679-20996aa99229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'peft'...\n",
            "remote: Enumerating objects: 5900, done.\u001b[K\n",
            "remote: Counting objects: 100% (1173/1173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (194/194), done.\u001b[K\n",
            "remote: Total 5900 (delta 1034), reused 1032 (delta 956), pack-reused 4727\u001b[K\n",
            "Receiving objects: 100% (5900/5900), 8.29 MiB | 22.29 MiB/s, done.\n",
            "Resolving deltas: 100% (3880/3880), done.\n",
            "/content/peft/examples/lora_dreambooth\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.35.2)\n",
            "Collecting accelerate (from -r requirements.txt (line 2))\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate (from -r requirements.txt (line 3))\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.66.1)\n",
            "Collecting datasets (from -r requirements.txt (line 5))\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers (from -r requirements.txt (line 6))\n",
            "  Downloading diffusers-0.24.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (9.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.16.0+cu118)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.4.1)\n",
            "Collecting wandb (from -r requirements.txt (line 11))\n",
            "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (0.15.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 2)) (2.1.0+cu118)\n",
            "Collecting dill (from evaluate->-r requirements.txt (line 3))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate->-r requirements.txt (line 3)) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate->-r requirements.txt (line 3)) (3.4.1)\n",
            "Collecting multiprocess (from evaluate->-r requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate->-r requirements.txt (line 3)) (2023.6.0)\n",
            "Collecting responses<0.19 (from evaluate->-r requirements.txt (line 3))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 5)) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets->-r requirements.txt (line 5))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 5)) (3.9.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->-r requirements.txt (line 6)) (6.8.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (2.1.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 11)) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 11))\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 11))\n",
            "  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 11))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb->-r requirements.txt (line 11))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 11)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 11)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 11)) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 11)) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 11))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.11.17)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->-r requirements.txt (line 6)) (3.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate->-r requirements.txt (line 3)) (2023.3.post1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 11))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.3.0)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, pyarrow-hotfix, docker-pycreds, dill, responses, multiprocess, gitdb, GitPython, diffusers, accelerate, wandb, datasets, evaluate\n",
            "Successfully installed GitPython-3.1.40 accelerate-0.25.0 datasets-2.15.0 diffusers-0.24.0 dill-0.3.7 docker-pycreds-0.4.0 evaluate-0.4.1 gitdb-4.0.11 multiprocess-0.70.15 pyarrow-hotfix-0.6 responses-0.18.0 sentry-sdk-1.38.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.0\n",
            "Collecting git+https://github.com/huggingface/peft\n",
            "  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-87hmah7p\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-87hmah7p\n",
            "  Resolved https://github.com/huggingface/peft to commit 6a57472665b2b712a84e2bedd98945038283f7cc\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.3.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.3.dev0) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.6.3.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.6.3.dev0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.3.dev0) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.6.3.dev0) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.6.3.dev0) (4.66.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.3.dev0) (0.25.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.6.3.dev0) (0.4.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.3.dev0) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.6.3.dev0) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.6.3.dev0) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.6.3.dev0) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.6.3.dev0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.3.dev0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.3.dev0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.3.dev0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.3.dev0) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.6.3.dev0) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.6.3.dev0) (0.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.6.3.dev0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.6.3.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.6.3.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.6.3.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.6.3.dev0) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.6.3.dev0) (1.3.0)\n",
            "Building wheels for collected packages: peft\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.6.3.dev0-py3-none-any.whl size=164682 sha256=b644de9f80d1a61b242cf206719dd2b32bed0716ba192cc080313f9188d14831\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8aqwl7zx/wheels/4c/16/67/1002a2d4daa822eff130e6d85b90051b75d2ce0d26b9448e4a\n",
            "Successfully built peft\n",
            "Installing collected packages: peft\n",
            "Successfully installed peft-0.6.3.dev0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!git clone https://github.com/huggingface/peft\n",
        "%cd peft/examples/lora_dreambooth\n",
        "!pip install -r requirements.txt\n",
        "!pip install git+https://github.com/huggingface/peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfocGVLxFdoQ",
        "outputId": "f8e9e7be-981f-4b1d-e1d5-b6583ed3d482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting peft==0.6.2\n",
            "  Downloading peft-0.6.2-py3-none-any.whl (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.7/174.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.2) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.6.2) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.6.2) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.2) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.6.2) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.6.2) (4.66.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.2) (0.25.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.6.2) (0.4.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->peft==0.6.2) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.2) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.2) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.2) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.2) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.2) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.6.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.6.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.6.2) (0.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.6.2) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.6.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.6.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.6.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.6.2) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.6.2) (1.3.0)\n",
            "Installing collected packages: peft\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.6.3.dev0\n",
            "    Uninstalling peft-0.6.3.dev0:\n",
            "      Successfully uninstalled peft-0.6.3.dev0\n",
            "Successfully installed peft-0.6.2\n"
          ]
        }
      ],
      "source": [
        "!pip install peft==0.6.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG8fZXmL1YLM",
        "outputId": "b1898ea3-42fd-4bf7-ef41-1689e78e0845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "12/02/2023 14:10:38 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "{'scaling_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "{'time_embedding_act_fn', 'time_embedding_dim', 'class_embeddings_concat', 'mid_block_only_cross_attention', 'time_cond_proj_dim', 'resnet_skip_time_act', 'addition_embed_type', 'attention_type', 'reverse_transformer_layers_per_block', 'mid_block_type', 'time_embedding_type', 'dropout', 'num_attention_heads', 'transformer_layers_per_block', 'timestep_post_act', 'conv_out_kernel', 'conv_in_kernel', 'encoder_hid_dim', 'only_cross_attention', 'num_class_embeds', 'addition_time_embed_dim', 'resnet_time_scale_shift', 'upcast_attention', 'class_embed_type', 'projection_class_embeddings_input_dim', 'addition_embed_type_num_heads', 'use_linear_projection', 'dual_cross_attention', 'cross_attention_norm', 'encoder_hid_dim_type', 'resnet_out_scale_factor'} was not found in config. Values will be initialized to default values.\n",
            "trainable params: 1,594,368 || all params: 861,115,332 || trainable%: 0.18515150535027286\n",
            "PeftModel(\n",
            "  (base_model): LoraModel(\n",
            "    (model): UNet2DConditionModel(\n",
            "      (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (time_proj): Timesteps()\n",
            "      (time_embedding): TimestepEmbedding(\n",
            "        (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
            "        (act): SiLU()\n",
            "        (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "      )\n",
            "      (down_blocks): ModuleList(\n",
            "        (0): CrossAttnDownBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-1): 2 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
            "              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): Linear(\n",
            "                      in_features=320, out_features=320, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
            "                    (to_v): Linear(\n",
            "                      in_features=320, out_features=320, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): Linear(\n",
            "                      in_features=320, out_features=320, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
            "                    (to_v): Linear(\n",
            "                      in_features=768, out_features=320, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0-1): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
            "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (downsamplers): ModuleList(\n",
            "            (0): Downsample2D(\n",
            "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): CrossAttnDownBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-1): 2 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
            "              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): Linear(\n",
            "                      in_features=640, out_features=640, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
            "                    (to_v): Linear(\n",
            "                      in_features=640, out_features=640, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): Linear(\n",
            "                      in_features=640, out_features=640, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
            "                    (to_v): Linear(\n",
            "                      in_features=768, out_features=640, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (1): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (downsamplers): ModuleList(\n",
            "            (0): Downsample2D(\n",
            "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): CrossAttnDownBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-1): 2 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): Linear(\n",
            "                      in_features=1280, out_features=1280, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                    (to_v): Linear(\n",
            "                      in_features=1280, out_features=1280, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): Linear(\n",
            "                      in_features=1280, out_features=1280, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                    (to_v): Linear(\n",
            "                      in_features=768, out_features=1280, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (1): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "          (downsamplers): ModuleList(\n",
            "            (0): Downsample2D(\n",
            "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): DownBlock2D(\n",
            "          (resnets): ModuleList(\n",
            "            (0-1): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (up_blocks): ModuleList(\n",
            "        (0): UpBlock2D(\n",
            "          (resnets): ModuleList(\n",
            "            (0-2): 3 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
            "              (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (upsamplers): ModuleList(\n",
            "            (0): Upsample2D(\n",
            "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): CrossAttnUpBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-2): 3 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): Linear(\n",
            "                      in_features=1280, out_features=1280, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                    (to_v): Linear(\n",
            "                      in_features=1280, out_features=1280, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): Linear(\n",
            "                      in_features=1280, out_features=1280, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                    (to_v): Linear(\n",
            "                      in_features=768, out_features=1280, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0-1): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
            "              (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (2): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
            "              (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (upsamplers): ModuleList(\n",
            "            (0): Upsample2D(\n",
            "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): CrossAttnUpBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-2): 3 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
            "              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): Linear(\n",
            "                      in_features=640, out_features=640, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
            "                    (to_v): Linear(\n",
            "                      in_features=640, out_features=640, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): Linear(\n",
            "                      in_features=640, out_features=640, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=640, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
            "                    (to_v): Linear(\n",
            "                      in_features=768, out_features=640, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=640, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
            "              (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (1): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "              (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (2): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
            "              (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (upsamplers): ModuleList(\n",
            "            (0): Upsample2D(\n",
            "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): CrossAttnUpBlock2D(\n",
            "          (attentions): ModuleList(\n",
            "            (0-2): 3 x Transformer2DModel(\n",
            "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
            "              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (transformer_blocks): ModuleList(\n",
            "                (0): BasicTransformerBlock(\n",
            "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn1): Attention(\n",
            "                    (to_q): Linear(\n",
            "                      in_features=320, out_features=320, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
            "                    (to_v): Linear(\n",
            "                      in_features=320, out_features=320, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attn2): Attention(\n",
            "                    (to_q): Linear(\n",
            "                      in_features=320, out_features=320, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=320, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
            "                    (to_v): Linear(\n",
            "                      in_features=768, out_features=320, bias=False\n",
            "                      (lora_dropout): ModuleDict(\n",
            "                        (default): Identity()\n",
            "                      )\n",
            "                      (lora_A): ModuleDict(\n",
            "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                      )\n",
            "                      (lora_B): ModuleDict(\n",
            "                        (default): Linear(in_features=16, out_features=320, bias=False)\n",
            "                      )\n",
            "                      (lora_embedding_A): ParameterDict()\n",
            "                      (lora_embedding_B): ParameterDict()\n",
            "                    )\n",
            "                    (to_out): ModuleList(\n",
            "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "                  (ff): FeedForward(\n",
            "                    (net): ModuleList(\n",
            "                      (0): GEGLU(\n",
            "                        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
            "                      )\n",
            "                      (1): Dropout(p=0.0, inplace=False)\n",
            "                      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (resnets): ModuleList(\n",
            "            (0): ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
            "              (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
            "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (1-2): 2 x ResnetBlock2D(\n",
            "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "              (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
            "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (nonlinearity): SiLU()\n",
            "              (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (mid_block): UNetMidBlock2DCrossAttn(\n",
            "        (attentions): ModuleList(\n",
            "          (0): Transformer2DModel(\n",
            "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (transformer_blocks): ModuleList(\n",
            "              (0): BasicTransformerBlock(\n",
            "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (attn1): Attention(\n",
            "                  (to_q): Linear(\n",
            "                    in_features=1280, out_features=1280, bias=False\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                  )\n",
            "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                  (to_v): Linear(\n",
            "                    in_features=1280, out_features=1280, bias=False\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                  )\n",
            "                  (to_out): ModuleList(\n",
            "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (attn2): Attention(\n",
            "                  (to_q): Linear(\n",
            "                    in_features=1280, out_features=1280, bias=False\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=1280, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                  )\n",
            "                  (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                  (to_v): Linear(\n",
            "                    in_features=768, out_features=1280, bias=False\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Identity()\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=16, out_features=1280, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                  )\n",
            "                  (to_out): ModuleList(\n",
            "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "                (ff): FeedForward(\n",
            "                  (net): ModuleList(\n",
            "                    (0): GEGLU(\n",
            "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                    )\n",
            "                    (1): Dropout(p=0.0, inplace=False)\n",
            "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "        (resnets): ModuleList(\n",
            "          (0-1): 2 x ResnetBlock2D(\n",
            "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (nonlinearity): SiLU()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "      (conv_act): SiLU()\n",
            "      (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "trainable params: 589,824 || all params: 123,650,304 || trainable%: 0.4770097451600281\n",
            "PeftModel(\n",
            "  (base_model): LoraModel(\n",
            "    (model): CLIPTextModel(\n",
            "      (text_model): CLIPTextTransformer(\n",
            "        (embeddings): CLIPTextEmbeddings(\n",
            "          (token_embedding): Embedding(49408, 768)\n",
            "          (position_embedding): Embedding(77, 768)\n",
            "        )\n",
            "        (encoder): CLIPEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-11): 12 x CLIPEncoderLayer(\n",
            "              (self_attn): CLIPAttention(\n",
            "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (v_proj): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Identity()\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "                (q_proj): Linear(\n",
            "                  in_features=768, out_features=768, bias=True\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Identity()\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              )\n",
            "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): CLIPMLP(\n",
            "                (activation_fn): QuickGELUActivation()\n",
            "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              )\n",
            "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2023-12-02 14:10:57.496066: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-02 14:10:57.496118: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-02 14:10:57.496158: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-02 14:10:59.013784: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "12/02/2023 14:11:00 - INFO - __main__ - ***** Running training *****\n",
            "12/02/2023 14:11:00 - INFO - __main__ -   Num examples = 200\n",
            "12/02/2023 14:11:00 - INFO - __main__ -   Num batches each epoch = 200\n",
            "12/02/2023 14:11:00 - INFO - __main__ -   Num Epochs = 4\n",
            "12/02/2023 14:11:00 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
            "12/02/2023 14:11:00 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "12/02/2023 14:11:00 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "12/02/2023 14:11:00 - INFO - __main__ -   Total optimization steps = 800\n",
            "Steps:   0% 0/800 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "Steps:  25% 200/800 [08:17<25:23,  2.54s/it, loss=0.335, lr=0.0001] GPU Memory before entering the train : 4094\n",
            "GPU Memory consumed at the end of the train (end-begin): 48\n",
            "GPU Peak Memory consumed during the train (max-begin): 1328\n",
            "GPU Total Peak Memory consumed during the train (max): 5422\n",
            "CPU Memory before entering the train : 2789\n",
            "CPU Memory consumed at the end of the train (end-begin): 1663\n",
            "CPU Peak Memory consumed during the train (max-begin): 1669\n",
            "CPU Total Peak Memory consumed during the train (max): 4458\n",
            "Steps:  50% 400/800 [16:41<17:11,  2.58s/it, loss=0.0854, lr=0.0001]GPU Memory before entering the train : 4143\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 1280\n",
            "GPU Total Peak Memory consumed during the train (max): 5423\n",
            "CPU Memory before entering the train : 4453\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 6\n",
            "CPU Total Peak Memory consumed during the train (max): 4459\n",
            "Steps:  75% 600/800 [25:04<08:29,  2.55s/it, loss=0.0624, lr=0.0001]GPU Memory before entering the train : 4143\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 1280\n",
            "GPU Total Peak Memory consumed during the train (max): 5423\n",
            "CPU Memory before entering the train : 4453\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 6\n",
            "CPU Total Peak Memory consumed during the train (max): 4459\n",
            "Steps: 100% 800/800 [33:27<00:00,  2.54s/it, loss=0.0445, lr=0.0001]GPU Memory before entering the train : 4143\n",
            "GPU Memory consumed at the end of the train (end-begin): 0\n",
            "GPU Peak Memory consumed during the train (max-begin): 1280\n",
            "GPU Total Peak Memory consumed during the train (max): 5423\n",
            "CPU Memory before entering the train : 4453\n",
            "CPU Memory consumed at the end of the train (end-begin): 0\n",
            "CPU Peak Memory consumed during the train (max-begin): 6\n",
            "CPU Total Peak Memory consumed during the train (max): 4459\n",
            "Steps: 100% 800/800 [33:28<00:00,  2.51s/it, loss=0.0445, lr=0.0001]\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n",
        "  --instance_data_dir=\"/content/drive/MyDrive/cs236 project cornell/text_inversion_hand_input2\" \\\n",
        "  --class_data_dir=\"/content/drive/MyDrive/cs236 project cornell/dreambooth_hand_class\" \\\n",
        "  --output_dir=\"/content/drive/MyDrive/cs236 project cornell/dreambooth_hand_output_prior_0_75\" \\\n",
        "  --train_text_encoder \\\n",
        "  --with_prior_preservation --prior_loss_weight=0.75 \\\n",
        "  --num_dataloader_workers=1 \\\n",
        "  --instance_prompt=\"a photo of sks hand\" \\\n",
        "  --class_prompt=\"a photo of hand\" \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=200 \\\n",
        "  --use_lora \\\n",
        "  --lora_r 16 \\\n",
        "  --lora_alpha 27 \\\n",
        "  --lora_text_encoder_r 16 \\\n",
        "  --lora_text_encoder_alpha 17 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --gradient_checkpointing \\\n",
        "  --max_train_steps=800"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}